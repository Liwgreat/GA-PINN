{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.566096Z",
     "start_time": "2024-02-06T16:20:00.341549Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "%matplotlib inline\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.572805Z",
     "start_time": "2024-02-06T16:20:01.568306Z"
    }
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        #nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.579486Z",
     "start_time": "2024-02-06T16:20:01.575584Z"
    }
   },
   "outputs": [],
   "source": [
    "class hidden_layers(nn.Module):\n",
    "    def __init__(self,input_number,output_number):\n",
    "        super(hidden_layers, self).__init__()\n",
    "        self.layer = nn.Linear(input_number,output_number)\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.587124Z",
     "start_time": "2024-02-06T16:20:01.581375Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN_H2 (nn.Module):\n",
    "    def __init__(self,in_N, width, depth, out_N):\n",
    "        #depth = layers-2\n",
    "        super(NN_H2, self).__init__()\n",
    "        self.in_N = in_N\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.out_N = out_N\n",
    "\n",
    "        self.stack = nn.ModuleList()\n",
    "\n",
    "        self.stack.append(hidden_layers(in_N, width))\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.stack.append(hidden_layers(width, width))\n",
    "\n",
    "        self.stack.append(nn.Linear(width, out_N))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for m in self.stack:\n",
    "            x = m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.595364Z",
     "start_time": "2024-02-06T16:20:01.589499Z"
    }
   },
   "outputs": [],
   "source": [
    "class get_discriminator(nn.Module):\n",
    "    def __init__(self,in_N, width, depth, out_N):\n",
    "        #depth = layers-2\n",
    "        super(get_discriminator, self).__init__()\n",
    "        self.in_N = in_N\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.out_N = out_N\n",
    "\n",
    "        self.stack = nn.ModuleList()\n",
    "\n",
    "        self.stack.append(hidden_layers(in_N, width))\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.stack.append(hidden_layers(width, width))\n",
    "\n",
    "        self.stack.append(nn.Linear(width, out_N))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for m in self.stack:\n",
    "            x = m(x)\n",
    "            x = torch.sigmoid(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.622236Z",
     "start_time": "2024-02-06T16:20:01.597477Z"
    }
   },
   "outputs": [],
   "source": [
    "N_train = 10000\n",
    "N_bound = 500\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "f_data = lhs(10,N_train)\n",
    "np.random.seed(1234)\n",
    "bound_x_all = lhs(10,N_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.631384Z",
     "start_time": "2024-02-06T16:20:01.624293Z"
    }
   },
   "outputs": [],
   "source": [
    "bound_x1 = bound_x_all.copy()\n",
    "bound_x1[:,0:1] = np.zeros((N_bound,1))\n",
    "bound_x2 = bound_x_all.copy()\n",
    "bound_x2[:,1:2] = np.zeros((N_bound,1))\n",
    "bound_x3 = bound_x_all.copy()\n",
    "bound_x3[:,2:3] = np.zeros((N_bound,1))\n",
    "bound_x4 = bound_x_all.copy()\n",
    "bound_x4[:,3:4] = np.zeros((N_bound,1))\n",
    "bound_x5 = bound_x_all.copy()\n",
    "bound_x5[:,4:5] = np.zeros((N_bound,1))\n",
    "bound_x6 = bound_x_all.copy()\n",
    "bound_x6[:,5:6] = np.zeros((N_bound,1))\n",
    "bound_x7 = bound_x_all.copy()\n",
    "bound_x7[:,6:7] = np.zeros((N_bound,1))\n",
    "bound_x8 = bound_x_all.copy()\n",
    "bound_x8[:,7:8] = np.zeros((N_bound,1))\n",
    "bound_x9 = bound_x_all.copy()\n",
    "bound_x9[:,8:9] = np.zeros((N_bound,1))\n",
    "bound_x10 = bound_x_all.copy()\n",
    "bound_x10[:,9:10] = np.zeros((N_bound,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.637022Z",
     "start_time": "2024-02-06T16:20:01.633468Z"
    }
   },
   "outputs": [],
   "source": [
    "bound_x_A = np.concatenate((bound_x1,bound_x2,bound_x3,bound_x4,bound_x5,bound_x6,bound_x7,bound_x8,bound_x9,bound_x10),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.649148Z",
     "start_time": "2024-02-06T16:20:01.640956Z"
    }
   },
   "outputs": [],
   "source": [
    "bound_x1 = bound_x_all.copy()\n",
    "bound_x1[:,0:1] = np.ones((N_bound,1))\n",
    "bound_x2 = bound_x_all.copy()\n",
    "bound_x2[:,1:2] = np.ones((N_bound,1))\n",
    "bound_x3 = bound_x_all.copy()\n",
    "bound_x3[:,2:3] = np.ones((N_bound,1))\n",
    "bound_x4 = bound_x_all.copy()\n",
    "bound_x4[:,3:4] = np.ones((N_bound,1))\n",
    "bound_x5 = bound_x_all.copy()\n",
    "bound_x5[:,4:5] = np.ones((N_bound,1))\n",
    "bound_x6 = bound_x_all.copy()\n",
    "bound_x6[:,5:6] = np.ones((N_bound,1))\n",
    "bound_x7 = bound_x_all.copy()\n",
    "bound_x7[:,6:7] = np.ones((N_bound,1))\n",
    "bound_x8 = bound_x_all.copy()\n",
    "bound_x8[:,7:8] = np.ones((N_bound,1))\n",
    "bound_x9 = bound_x_all.copy()\n",
    "bound_x9[:,8:9] = np.ones((N_bound,1))\n",
    "bound_x10 = bound_x_all.copy()\n",
    "bound_x10[:,9:10] = np.ones((N_bound,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.655265Z",
     "start_time": "2024-02-06T16:20:01.651299Z"
    }
   },
   "outputs": [],
   "source": [
    "bound_x_B = np.concatenate((bound_x1,bound_x2,bound_x3,bound_x4,bound_x5,bound_x6,bound_x7,bound_x8,bound_x9,bound_x10),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.660228Z",
     "start_time": "2024-02-06T16:20:01.657350Z"
    }
   },
   "outputs": [],
   "source": [
    "bound_x = np.concatenate((bound_x_A,bound_x_B),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.666404Z",
     "start_time": "2024-02-06T16:20:01.662029Z"
    }
   },
   "outputs": [],
   "source": [
    "x1= f_data[:,0:1]\n",
    "x2= f_data[:,1:2]\n",
    "x3= f_data[:,2:3]\n",
    "x4= f_data[:,3:4]\n",
    "x5= f_data[:,4:5]\n",
    "x6= f_data[:,5:6]\n",
    "x7= f_data[:,6:7]\n",
    "x8= f_data[:,7:8]\n",
    "x9= f_data[:,8:9]\n",
    "x10= f_data[:,9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.692084Z",
     "start_time": "2024-02-06T16:20:01.668342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = torch.from_numpy(x1).float()\n",
    "x2 = torch.from_numpy(x2).float()\n",
    "x3 = torch.from_numpy(x3).float()\n",
    "x4 = torch.from_numpy(x4).float()\n",
    "x5 = torch.from_numpy(x5).float()\n",
    "x6 = torch.from_numpy(x6).float()\n",
    "x7 = torch.from_numpy(x7).float()\n",
    "x8 = torch.from_numpy(x8).float()\n",
    "x9 = torch.from_numpy(x9).float()\n",
    "x10 = torch.from_numpy(x10).float()\n",
    "bound_x = torch.from_numpy(bound_x).float()\n",
    "x1.requires_grad_()\n",
    "x2.requires_grad_()\n",
    "x3.requires_grad_()\n",
    "x4.requires_grad_()\n",
    "x5.requires_grad_()\n",
    "x6.requires_grad_()\n",
    "x7.requires_grad_()\n",
    "x8.requires_grad_()\n",
    "x9.requires_grad_()\n",
    "x10.requires_grad_()\n",
    "bound_x.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.696169Z",
     "start_time": "2024-02-06T16:20:01.693459Z"
    }
   },
   "outputs": [],
   "source": [
    "def relative_l2(u_pred,u_real):\n",
    "    l2 = np.linalg.norm(u_real-u_pred,2)/np.linalg.norm(u_real,2)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.722631Z",
     "start_time": "2024-02-06T16:20:01.697767Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(5678)\n",
    "test_data = lhs(10,N_train)\n",
    "test_data_tensor = torch.from_numpy(test_data).float()\n",
    "r = test_data[:,0:1]**2-test_data[:,1:2]**2+test_data[:,2:3]**2-test_data[:,3:4]**2+test_data[:,4:5]*test_data[:,5:6]+test_data[:,6:7]*test_data[:,7:8]*test_data[:,8:9]*test_data[:,9:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:20:01.729231Z",
     "start_time": "2024-02-06T16:20:01.724263Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(5678)\n",
    "gan_data = lhs(10,100)\n",
    "gan_data = torch.from_numpy(gan_data).float()\n",
    "gan_data_u = gan_data[:,0:1]**2-gan_data[:,1:2]**2+gan_data[:,2:3]**2-gan_data[:,3:4]**2+gan_data[:,4:5]*gan_data[:,5:6]+gan_data[:,6:7]*gan_data[:,7:8]*gan_data[:,8:9]*gan_data[:,9:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method4 \n",
    "$\\text { GA - PINN }^{\\dagger}$\n",
    "\n",
    "<div class=\"alert alert-info\">`loss_function：`\n",
    "\n",
    "$\n",
    "\\mathrm{L}_D=\\frac{1}{J} \\sum_{j=1}^J\\left(1-D\\left[\\left(\\mathbf{x}_T^{(j)}, u_T^{(j)}\\right)\\right]\\right)+D\\left[\\left(x_L^{(j)}, G\\left[x_L^{(j)}\\right]\\right)\\right] \\\\\n",
    "\\mathrm{L}_G=\\mathrm{L}_T+\\frac{1}{J} \\sum_{j=1}^J\\left(1-D\\left[\\left(x_T^{(j)}, G\\left[x_T^{(j)}\\right]\\right)\\right]\\right)\\\\\n",
    "\\overline{\\mathrm{L}}_{\\text {PINN }} =\\mathrm{L}_{\\text {PINN }}+\\lambda_2\\mathrm{L}_T\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T16:27:37.236861Z",
     "start_time": "2024-02-06T16:20:01.731169Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "PINNs4 = NN_H2(10,100,4, 1)\n",
    "PINNs4.apply(weights_init)\n",
    "optimizer1 = optim.Adam(PINNs4.parameters(), lr=0.001)\n",
    "\n",
    "discriminator = get_discriminator(11, 100, 1, 1)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "optimizer2 = optim.Adam([{'params': discriminator.parameters(), 'weight_decay': 0.001}], lr=5e-3)\n",
    "\n",
    "\n",
    "#########gpu#############\n",
    "discriminator = discriminator.cuda()\n",
    "bound_x = bound_x.cuda()\n",
    "gan_data = gan_data.cuda()\n",
    "gan_data_u = gan_data_u.cuda()\n",
    "x1 = x1.cuda()\n",
    "x2 = x2.cuda()\n",
    "x3 = x3.cuda()\n",
    "x4 = x4.cuda()\n",
    "x5 = x5.cuda()\n",
    "x6 = x6.cuda()\n",
    "x7 = x7.cuda()\n",
    "x8 = x8.cuda()\n",
    "x9 = x9.cuda()\n",
    "x10 = x10.cuda()\n",
    "#########gpu#############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_all_4 = []\n",
    "test_loss_4 = []\n",
    "\n",
    "loss1_value = 1\n",
    "it = 0\n",
    "while loss1_value > 0.002 :\n",
    "    PINNs4.cuda()\n",
    "    ##########loss D#################\n",
    "    pre_H = PINNs4(gan_data)\n",
    "    d_fake = discriminator(torch.cat((gan_data,pre_H.detach()),1))\n",
    "    d_real = discriminator(torch.cat((gan_data,gan_data_u),1))\n",
    "    loss_d = torch.mean(1-d_real)+torch.mean(d_fake)\n",
    "    \n",
    "    #######gpu#########\n",
    "    loss_d = loss_d.cuda()\n",
    "    \n",
    "    #######gpu#########\n",
    "    \n",
    "    \n",
    "    optimizer2.zero_grad()\n",
    "    loss_d .backward()\n",
    "    optimizer2.step()    \n",
    "    \n",
    "    #########loss G#################\n",
    "    for param_group in optimizer1.param_groups:\n",
    "        param_group[\"lr\"]=1e-3\n",
    "    pre_H = PINNs4(gan_data)\n",
    "    d_fake = discriminator(torch.cat((gan_data,pre_H),1))\n",
    "        \n",
    "    loss_L = torch.mean(1-d_fake)+torch.mean(torch.square(pre_H - gan_data_u))\n",
    "    \n",
    "    #######gpu#########\n",
    "    loss_L = loss_L.cuda()\n",
    "    \n",
    "    #######gpu#########\n",
    "    \n",
    "    optimizer1.zero_grad()\n",
    "    loss_L.backward()\n",
    "    optimizer1.step()    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######### loss PINN############\n",
    "    for param_group in optimizer1.param_groups:\n",
    "        param_group[\"lr\"]=1e-3\n",
    "    \n",
    "    ##### loss_Bi  ######\n",
    "    E_bound = PINNs4(bound_x)\n",
    "    #real_bound = bound_x[:,0:1]*bound_x[:,1:2]+bound_x[:,2:3]*bound_x[:,3:4]+bound_x[:,4:5]*bound_x[:,5:6]+bound_x[:,6:7]*bound_x[:,7:8]+bound_x[:,8:9]*bound_x[:,9:10]\n",
    "    real_bound = bound_x[:,0:1]**2-bound_x[:,1:2]**2+bound_x[:,2:3]**2-bound_x[:,3:4]**2+bound_x[:,4:5]*bound_x[:,5:6]+bound_x[:,6:7]*bound_x[:,7:8]*bound_x[:,8:9]*bound_x[:,9:10]\n",
    "    loss_bound = torch.mean(torch.square(E_bound-real_bound))\n",
    "    ##### loss f  ######\n",
    "    \n",
    "    E = PINNs4(torch.cat((x1,x2,x3,x4,x5,x6,x7,x8,x9,x10),1))\n",
    "    E_x1 = autograd.grad(outputs=E, inputs=x1,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx1 = autograd.grad(outputs=E_x1, inputs=x1,\n",
    "                              grad_outputs=torch.ones_like(E_x1),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_x2 = autograd.grad(outputs=E, inputs=x2,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx2 = autograd.grad(outputs=E_x2, inputs=x2,\n",
    "                              grad_outputs=torch.ones_like(E_x2),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_x3 = autograd.grad(outputs=E, inputs=x3,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx3 = autograd.grad(outputs=E_x3, inputs=x3,\n",
    "                              grad_outputs=torch.ones_like(E_x3),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_x4 = autograd.grad(outputs=E, inputs=x4,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx4 = autograd.grad(outputs=E_x4, inputs=x4,\n",
    "                              grad_outputs=torch.ones_like(E_x4),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_x5 = autograd.grad(outputs=E, inputs=x5,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx5 = autograd.grad(outputs=E_x5, inputs=x5,\n",
    "                              grad_outputs=torch.ones_like(E_x5),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_x6 = autograd.grad(outputs=E, inputs=x6,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx6 = autograd.grad(outputs=E_x6, inputs=x6,\n",
    "                              grad_outputs=torch.ones_like(E_x6),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]    \n",
    "    E_x7 = autograd.grad(outputs=E, inputs=x7,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx7 = autograd.grad(outputs=E_x7, inputs=x7,\n",
    "                              grad_outputs=torch.ones_like(E_x7),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]      \n",
    "    E_x8 = autograd.grad(outputs=E, inputs=x8,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx8 = autograd.grad(outputs=E_x8, inputs=x8,\n",
    "                              grad_outputs=torch.ones_like(E_x8),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]    \n",
    "    E_x9 = autograd.grad(outputs=E, inputs=x9,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx9 = autograd.grad(outputs=E_x9, inputs=x9,\n",
    "                              grad_outputs=torch.ones_like(E_x9),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]    \n",
    "    E_x10 = autograd.grad(outputs=E, inputs=x10,\n",
    "                              grad_outputs=torch.ones_like(E),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    E_xx10 = autograd.grad(outputs=E_x10, inputs=x1,\n",
    "                              grad_outputs=torch.ones_like(E_x10),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]       \n",
    "    loss_f = torch.mean(torch.square(E_xx1+E_xx2+E_xx3+E_xx4+E_xx5+E_xx6+E_xx7+E_xx8+E_xx9+E_xx10))\n",
    "    \n",
    "    \n",
    "    pre_H = PINNs4(gan_data)\n",
    "    loss_p = loss_bound+loss_f+torch.mean(torch.square(pre_H - gan_data_u)) \n",
    "    loss = loss_bound+loss_f \n",
    "\n",
    "    loss_p = loss_p.cuda()    \n",
    "     \n",
    "        \n",
    "    loss1_value = loss.item()\n",
    "    \n",
    "    loss_all_4.append(loss1_value)\n",
    "    optimizer1.zero_grad()\n",
    "    loss_p.backward()\n",
    "    optimizer1.step()\n",
    "    \n",
    "    #########  test_loss NRMSE  #########\n",
    "    PINNs4.cpu()\n",
    "    e1 = relative_l2(PINNs4(test_data_tensor).detach().numpy(),r)\n",
    "    test_loss_4.append(e1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if it % 100 == 0:\n",
    "        print('It:', it, 'Loss:', loss.item())\n",
    "    it = it + 1        \n",
    "loss1_value    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T14:47:45.001946Z",
     "start_time": "2024-10-28T14:47:44.943375Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save('../experimental_data/method_4/test_loss_4',test_loss_4)\n",
    "# np.save('../experimental_data/method_4/loss_all_4',loss_all_4)\n",
    "# torch.save(PINNs4,'../saved_model/PINNs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch and NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T14:47:45.219216Z",
     "start_time": "2024-10-28T14:47:45.209886Z"
    }
   },
   "outputs": [],
   "source": [
    "Epochs = len(test_loss_4)\n",
    "NRMSE = relative_l2(PINNs4(test_data_tensor).detach().numpy(),r)\n",
    "\n",
    "print('Epochs:',Epochs,'NRMSE:',NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GA-PINNs",
   "language": "python",
   "name": "gapings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
